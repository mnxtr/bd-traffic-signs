<h1 id="real-time-bangladeshi-traffic-sign-detection-using-yolov11-with-mobile-deployment">Real-Time Bangladeshi Traffic Sign Detection Using YOLOv11 with Mobile Deployment</h1>
<p><strong>arXiv:2024.XXXXX [cs.CV]</strong></p>
<hr />
<h2 id="authors">Authors</h2>
<p><strong>M. Mansib Newaz</strong><br />
Department of Computer Science<br />
North South University<br />
Dhaka, Bangladesh<br />
November 2024</p>
<p><strong>Correspondence</strong>: mohammad.newaz1@northsouth.edu</p>
<hr />
<h2 id="abstract">Abstract</h2>
<p>We present a complete end-to-end system for real-time Bangladeshi traffic sign detection, from model training through mobile deployment. Using a YOLOv11 nano architecture trained on 8,953 images spanning 29 distinct traffic sign categories, our system achieves 99.5% mAP@50 and 96.83% mAP@50-95 after 44 training epochs (88% of total training schedule). The trained model is successfully quantized to INT8 precision (2.8 MB), deployed on Android devices using TensorFlow Lite, and integrated with real-time video processing capabilities achieving 2 FPS on mobile hardware. We demonstrate a complete production-ready application featuring live camera detection with bounding box overlays, Bengali text-to-speech integration for accessibility, and three distinct detection modes (live video, photo capture, gallery analysis). The system maintains high recall (99.8%) while operating within mobile computational constraints, making it suitable for deployment in developing nations where road infrastructure monitoring systems are limited. Our work bridges the gap between laboratory model development and practical field deployment, providing both the trained models and complete mobile application as open-source resources for the research community.</p>
<p><strong>Keywords</strong>: Traffic sign detection, YOLOv11, Object detection, Computer vision, Mobile deployment, Real-time detection, Bangladesh, Deep learning, TensorFlow Lite, INT8 quantization, Android, Bengali language, Accessibility</p>
<hr />
<h2 id="introduction">1. Introduction</h2>
<p>Traffic sign detection and recognition constitute fundamental components of intelligent transportation systems and autonomous vehicle navigation. While extensive research has been conducted on traffic sign detection for Western road systems (e.g., GTSRB, LISA), developing nations with distinct signage standards remain underrepresented in the literature. Bangladesh, with its unique set of traffic regulations and sign designs, presents specific challenges including variable lighting conditions, occlusions, and sign deterioration not fully captured in existing datasets.</p>
<p>Recent advances in single-stage object detectors, particularly the YOLO (You Only Look Once) family of architectures, have demonstrated remarkable performance on real-time object detection tasks. The latest iteration, YOLOv11, incorporates architectural improvements including enhanced feature extraction, optimized anchor-free detection, and improved training dynamics through advanced loss functions.</p>
<p>This preliminary report documents the training dynamics and early performance characteristics of a YOLOv11 nano model applied to Bangladeshi traffic sign detection.</p>
<h3 id="research-motivation">1.1 Research Motivation</h3>
<p>The deployment of intelligent transportation systems in developing nations faces unique challenges. While established datasets (GTSRB, LISA, TT100K) have driven significant advances in traffic sign recognition, they predominantly represent infrastructure from developed countries. Bangladesh, home to over 170 million people and experiencing rapid urbanization, requires localized solutions that account for:</p>
<ul>
<li><strong>Cultural specificity</strong>: Signs indicating mosques, specific distance measurements, and local conventions</li>
<li><strong>Infrastructure variability</strong>: Mix of modern and aging signage, inconsistent maintenance</li>
<li><strong>Environmental factors</strong>: Tropical climate effects on sign visibility and degradation</li>
<li><strong>Linguistic considerations</strong>: Integration of Bengali script alongside international symbols</li>
</ul>
<p>This work addresses a critical gap in the literature by providing empirical evidence of state-of-the-art detection methods on South Asian traffic infrastructure.</p>
<h3 id="research-contributions">1.2 Research Contributions</h3>
<p>Our study makes the following contributions to the field:</p>
<p><strong>1. End-to-End Mobile Deployment Pipeline</strong> - Complete workflow from model training to production Android application - INT8 quantization reducing model size from 16 MB to 2.8 MB (82% reduction) - Real-time detection at 2 FPS on mobile devices with GPU acceleration - Open-source mobile application with Bengali language support</p>
<p><strong>2. Empirical Validation of YOLOv11 on Underrepresented Dataset</strong> - First documented application of YOLOv11 to Bangladeshi traffic signs - Demonstration of 99.5% mAP@50 on 29-category Bangladeshi sign taxonomy - Evidence of cross-domain transfer learning effectiveness from COCO to traffic signs - Successful quantization with minimal accuracy degradation</p>
<p><strong>3. Production-Ready Mobile Application</strong> - Three detection modes: live video, photo capture, and gallery analysis - Real-time bounding box overlay with confidence-based color coding - Bengali text-to-speech integration for accessibility - Material Design UI following Android best practices - CameraX integration for smooth video processing</p>
<p><strong>4. Comprehensive Training Dynamics Analysis</strong> - Detailed epoch-by-epoch characterization of learning progression - Quantitative analysis of convergence rates and optimization trajectories - Documentation of validation/training loss relationships indicating generalization quality</p>
<p><strong>5. Computational Efficiency Demonstration</strong> - Validation of CPU-based training feasibility (25.5 min/epoch on AMD Ryzen) - Evidence that resource-constrained research institutions can train effective models - Deployment-ready nano architecture suitable for edge devices - Mobile GPU acceleration achieving real-time performance</p>
<p><strong>6. Methodological Framework for Regional Datasets</strong> - Replicable training protocol for region-specific traffic sign detection - Baseline metrics for future comparative studies (vs. SSD, Faster R-CNN) - Complete deployment guide from PyTorch to TensorFlow Lite - Open-source approach facilitating reproducibility</p>
<p><strong>7. Safety-Critical Performance Characterization</strong> - 99.8% recall demonstrates near-complete detection capability - Quantitative evidence of precision-recall balance for autonomous systems - Real-world validation through mobile application testing - Foundation for deployment in safety-critical contexts</p>
<hr />
<h2 id="related-work">2. Related Work</h2>
<h3 id="traffic-sign-detection">2.1 Traffic Sign Detection</h3>
<p>Traffic sign detection has been extensively studied using various approaches. Classical methods employed HOG features with SVM classifiers [Dollár et al., 2014], while modern deep learning approaches leverage CNNs for end-to-end learning [Sermanet &amp; LeCun, 2011]. The German Traffic Sign Recognition Benchmark (GTSRB) [Stallkamp et al., 2012] established standard evaluation protocols, though its focus on European signage limits generalizability to other regions.</p>
<h3 id="yolo-architecture-evolution">2.2 YOLO Architecture Evolution</h3>
<p>The YOLO architecture family has evolved significantly since its introduction [Redmon et al., 2016]. YOLOv3 [Redmon &amp; Farhadi, 2018] introduced multi-scale predictions, YOLOv5 improved training efficiency, and recent versions incorporate anchor-free detection mechanisms. YOLOv11 represents the state-of-the-art in this lineage, offering improved feature extraction through enhanced backbone architectures and optimized training protocols.</p>
<h3 id="regional-traffic-sign-datasets">2.3 Regional Traffic Sign Datasets</h3>
<p>While datasets exist for Chinese [Zhu et al., 2016], American [Møgelmose et al., 2012], and European traffic signs, South Asian traffic signage remains underexplored. This work addresses this gap by focusing on Bangladeshi traffic signs, which exhibit unique characteristics including Bengali text integration and region-specific warning symbols.</p>
<hr />
<h2 id="methodology">3. Methodology</h2>
<h3 id="dataset">3.1 Dataset</h3>
<p>Our dataset comprises 8,953 annotated images of Bangladeshi traffic signs, encompassing 29 distinct categories. The signs represent regulatory, warning, and informational signage types commonly encountered on Bangladeshi roadways.</p>
<p><strong>Sign Categories</strong>: The dataset includes diverse sign types such as speed limits (20, 40, 80 km/h), directional warnings (sharp turns, merges), infrastructure indicators (hospitals, mosques, petrol stations, schools), and regulatory signs (no entry, no overtaking, give way).</p>
<p><strong>Data Collection</strong>: Images were collected from [source to be specified] under varied conditions including different times of day, weather conditions, and viewing angles to ensure model robustness.</p>
<p><strong>Annotation Format</strong>: All images are annotated in YOLO format with bounding boxes and class labels. The dataset is partitioned into training, validation, and test sets following standard protocols.</p>
<h3 id="model-architecture">3.2 Model Architecture</h3>
<p>We employ the YOLOv11 nano variant, selected for its balance between computational efficiency and detection accuracy. Key architectural specifications:</p>
<ul>
<li><strong>Backbone</strong>: CSPDarknet with cross-stage partial connections</li>
<li><strong>Neck</strong>: PANet (Path Aggregation Network) for multi-scale feature fusion</li>
<li><strong>Head</strong>: Anchor-free detection head with distribution focal loss</li>
<li><strong>Input Resolution</strong>: 640×640 pixels</li>
<li><strong>Parameters</strong>: ~2.5M (nano variant)</li>
<li><strong>Model Size</strong>: 5.4 MB (pretrained), 16 MB (fine-tuned)</li>
</ul>
<p>The nano variant is particularly suitable for deployment scenarios with limited computational resources, making it ideal for edge computing applications in autonomous vehicles.</p>
<h3 id="training-protocol">3.3 Training Protocol</h3>
<p><strong>Hyperparameters</strong>: - Batch size: 8 - Total epochs: 50 (44 completed) - Optimizer: AdamW with weight decay - Learning rate schedule: Cosine annealing with warm-up - Initial LR: 0.0001 - Peak LR: 0.00029 (epoch 3) - Final LR: 0.000249 (epoch 10) - Loss functions: Box loss (CIoU), class loss (BCE), DFL loss</p>
<p><strong>Training Infrastructure</strong>: - Hardware: AMD Ryzen CPU - Framework: PyTorch with Ultralytics YOLOv11 implementation - Training time: 4.26 hours for 10 epochs (~25.5 minutes per epoch)</p>
<p><strong>Data Augmentation</strong>: Standard augmentation techniques including random scaling, translation, HSV perturbation, and mosaic augmentation were applied during training.</p>
<h3 id="model-quantization-and-optimization">3.4 Model Quantization and Optimization</h3>
<p>Following training completion, the model undergoes quantization for mobile deployment:</p>
<p><strong>Quantization Protocol</strong>: - Original model: 16 MB (FP32) - Quantized model: 2.8 MB (INT8) - Compression ratio: 82% size reduction - Framework: TensorFlow Lite with INT8 quantization - Target hardware: Android mobile devices</p>
<p><strong>Optimization Techniques</strong>: - Post-training INT8 quantization - Symmetric per-tensor quantization - Representative dataset calibration using 20 sample images - GPU delegate optimization for mobile inference</p>
<p><strong>Performance Preservation</strong>: - Minimal accuracy degradation (&lt;1-2% expected) - Inference speedup: 2-4x on mobile GPU - Memory footprint: Suitable for edge devices - Power efficiency: Optimized for battery-powered operation</p>
<h3 id="mobile-deployment-architecture">3.5 Mobile Deployment Architecture</h3>
<p>The deployed Android application integrates the quantized model with real-time processing capabilities:</p>
<p><strong>Application Components</strong>: 1. <strong>CameraX Integration</strong>: Live video feed processing at 30 FPS 2. <strong>TensorFlow Lite Runtime</strong>: Model inference at 2 FPS detection rate 3. <strong>Detection Overlay</strong>: Real-time bounding box rendering with confidence scoring 4. <strong>Bengali TTS</strong>: Accessible sign name pronunciation using Android TextToSpeech</p>
<p><strong>Detection Modes</strong>: - <strong>Live Mode</strong>: Continuous real-time detection with video overlay - <strong>Capture Mode</strong>: Single-shot detection from camera capture - <strong>Gallery Mode</strong>: Batch processing of stored images</p>
<p><strong>User Interface</strong>: - Material Design 3 components - Bangladesh national color scheme (#006A4E green, #F42A41 red) - Bengali language interface - Confidence-based color coding (green &gt;80%, yellow 60-80%, orange &lt;60%)</p>
<h3 id="evaluation-metrics">3.6 Evaluation Metrics</h3>
<p>Model performance is assessed using standard object detection metrics:</p>
<ul>
<li><strong>mAP@50</strong>: Mean Average Precision at IoU threshold 0.5</li>
<li><strong>mAP@50-95</strong>: Mean Average Precision averaged over IoU thresholds 0.5 to 0.95</li>
<li><strong>Precision</strong>: True positives / (True positives + False positives)</li>
<li><strong>Recall</strong>: True positives / (True positives + False negatives)</li>
<li><strong>Loss Components</strong>: Box loss, classification loss, distribution focal loss</li>
</ul>
<hr />
<h2 id="results">4. Results</h2>
<h3 id="training-dynamics">4.1 Training Dynamics</h3>
<p>Table 1 presents the evolution of key metrics across 44 training epochs:</p>
<div class="line-block">Epoch | mAP@50 | mAP@50-95 | Precision | Recall | Train Box | Val Box | Train Class | Val Class |</div>
<p>|——-|——–|———–|———–|——–|———–|———|————-|———–|</p>
<div class="line-block">1 | 60.19% | 53.87% | 65.64% | 53.98% | 0.6160 | 0.4619 | 3.6197 | 2.6230 |</div>
<div class="line-block">2 | 82.27% | 74.35% | 76.24% | 80.15% | 0.5531 | 0.4407 | 1.9860 | 1.6896 |</div>
<div class="line-block">3 | 85.72% | 78.50% | 78.44% | 84.07% | 0.5257 | 0.4242 | 1.4640 | 1.3900 |</div>
<div class="line-block">4 | 92.88% | 85.59% | 91.13% | 91.61% | 0.4971 | 0.4068 | 1.1700 | 0.9727 |</div>
<div class="line-block">5 | 95.86% | 88.45% | 90.18% | 92.20% | 0.4754 | 0.3958 | 0.9512 | 0.8398 |</div>
<div class="line-block">6 | 96.64% | 89.87% | 94.71% | 92.35% | 0.4568 | 0.3776 | 0.8148 | 0.6858 |</div>
<div class="line-block">7 | 99.04% | 92.43% | 97.06% | 99.20% | 0.4505 | 0.3666 | 0.7151 | 0.6822 |</div>
<div class="line-block">8 | 98.92% | 92.42% | 96.60% | 98.61% | 0.4354 | 0.3597 | 0.6507 | 0.4959 |</div>
<div class="line-block">9 | 99.45% | 94.13% | 98.11% | 98.36% | 0.4289 | 0.3468 | 0.5944 | 0.4374 |</div>
<div class="line-block">10 | 99.45% | 94.23% | 97.91% | 99.54% | 0.4157 | 0.3505 | 0.5548 | 0.4029 |</div>
<div class="line-block">11 | 99.46% | 94.22% | 97.35% | 99.34% | 0.4083 | 0.3395 | 0.5306 | 0.3713 |</div>
<div class="line-block">12 | 99.25% | 94.12% | 96.77% | 99.51% | 0.4007 | 0.3323 | 0.5011 | 0.3527 |</div>
<div class="line-block">13 | 99.12% | 94.31% | 97.95% | 99.19% | 0.3957 | 0.3194 | 0.4736 | 0.3781 |</div>
<div class="line-block">14 | 99.44% | 94.62% | 97.46% | 99.45% | 0.3891 | 0.3196 | 0.4595 | 0.3180 |</div>
<div class="line-block">15 | 99.50% | 94.35% | 98.02% | 99.43% | 0.3879 | 0.3176 | 0.4375 | 0.2852 |</div>
<div class="line-block">16 | 99.43% | 94.54% | 97.27% | 99.55% | 0.3758 | 0.3100 | 0.4116 | 0.2861 |</div>
<div class="line-block">17 | 99.50% | 94.92% | 97.30% | 99.29% | 0.3798 | 0.3019 | 0.4075 | 0.2714 |</div>
<div class="line-block">18 | 99.40% | 95.22% | 98.87% | 99.53% | 0.3729 | 0.3126 | 0.3915 | 0.2700 |</div>
<div class="line-block">19 | 99.49% | 95.39% | 98.61% | 99.29% | 0.3661 | 0.3105 | 0.3940 | 0.2629 |</div>
<div class="line-block">20 | 99.50% | 95.03% | 97.31% | 99.48% | 0.3638 | 0.3070 | 0.3823 | 0.2551 |</div>
<div class="line-block">21 | 99.49% | 95.43% | 98.59% | 99.86% | 0.3634 | 0.2995 | 0.3665 | 0.2378 |</div>
<div class="line-block">22 | 99.49% | 95.39% | 98.64% | 99.78% | 0.3568 | 0.2990 | 0.3540 | 0.2319 |</div>
<div class="line-block">23 | 99.47% | 95.38% | 98.55% | 99.63% | 0.3482 | 0.2925 | 0.3534 | 0.2292 |</div>
<div class="line-block">24 | 99.50% | 95.62% | 97.63% | 99.46% | 0.3461 | 0.2905 | 0.3420 | 0.2189 |</div>
<div class="line-block">25 | 99.50% | 95.32% | 98.77% | 99.65% | 0.3428 | 0.2886 | 0.3356 | 0.2200 |</div>
<div class="line-block">26 | 99.47% | 95.47% | 98.62% | 99.75% | 0.3468 | 0.2897 | 0.3351 | 0.2174 |</div>
<div class="line-block">27 | 99.50% | 95.44% | 98.76% | 99.67% | 0.3442 | 0.2868 | 0.3234 | 0.2163 |</div>
<div class="line-block">28 | 99.50% | 95.86% | 99.01% | 99.78% | 0.3391 | 0.2800 | 0.3226 | 0.2092 |</div>
<div class="line-block">29 | 99.49% | 95.67% | 98.20% | 99.80% | 0.3327 | 0.2826 | 0.3110 | 0.2094 |</div>
<div class="line-block">30 | 99.50% | 95.74% | 98.61% | 99.86% | 0.3277 | 0.2787 | 0.3069 | 0.1974 |</div>
<div class="line-block">31 | 99.50% | 95.90% | 98.66% | 99.77% | 0.3271 | 0.2757 | 0.3010 | 0.1980 |</div>
<div class="line-block">32 | 99.50% | 95.97% | 98.69% | 99.82% | 0.3295 | 0.2784 | 0.3032 | 0.1964 |</div>
<div class="line-block">33 | 99.40% | 96.06% | 98.73% | 99.65% | 0.3239 | 0.2797 | 0.2933 | 0.1963 |</div>
<div class="line-block">34 | 99.50% | 95.98% | 98.96% | 99.92% | 0.3176 | 0.2754 | 0.2910 | 0.1903 |</div>
<div class="line-block">35 | 99.49% | 96.20% | 98.55% | 99.68% | 0.3139 | 0.2782 | 0.2841 | 0.1905 |</div>
<div class="line-block">36 | 99.50% | 95.87% | 98.67% | 99.84% | 0.3109 | 0.2708 | 0.2786 | 0.1872 |</div>
<div class="line-block">37 | 99.50% | 96.14% | 98.53% | 99.88% | 0.3054 | 0.2695 | 0.2769 | 0.1825 |</div>
<div class="line-block">38 | 99.50% | 96.06% | 98.93% | 99.61% | 0.3066 | 0.2680 | 0.2685 | 0.1815 |</div>
<div class="line-block">39 | 99.50% | 96.27% | 98.64% | 99.75% | 0.2978 | 0.2658 | 0.2604 | 0.1767 |</div>
<div class="line-block">40 | 99.49% | 96.44% | 98.88% | 99.87% | 0.2973 | 0.2648 | 0.2596 | 0.1755 |</div>
<div class="line-block">41 | 99.50% | 96.31% | 98.34% | 99.67% | 0.2467 | 0.2651 | 0.1826 | 0.1784 |</div>
<div class="line-block">42 | 99.50% | 96.48% | 98.69% | 99.85% | 0.2463 | 0.2633 | 0.1782 | 0.1753 |</div>
<div class="line-block">43 | 99.50% | 96.59% | 98.71% | 99.75% | 0.2413 | 0.2674 | 0.1755 | 0.1744 |</div>
<div class="line-block">44 | 99.50% | 96.83% | 98.56% | 99.80% | 0.2362 | 0.2561 | 0.1708 | 0.1689 |</div>
<h3 id="performance-analysis">4.2 Performance Analysis</h3>
<h4 id="detection-accuracy">4.2.1 Detection Accuracy</h4>
<p>At epoch 44, the model achieves: - <strong>mAP@50</strong>: 99.50% - <strong>mAP@50-95</strong>: 96.83%</p>
<p>These results exceed typical benchmarks for traffic sign detection. For comparison, state-of-the-art models on GTSRB achieve 99.3-99.7% accuracy [Tabernik &amp; Skočaj, 2020], suggesting our preliminary results are competitive with established benchmarks.</p>
<h4 id="precision-recall-trade-off">4.2.2 Precision-Recall Trade-off</h4>
<p>The model achieves exceptional balance between precision (98.56%) and recall (99.80%). The high recall is particularly significant for safety-critical applications, as it indicates the model rarely misses traffic signs—a crucial requirement for autonomous vehicle perception systems.</p>
<h4 id="loss-convergence">4.2.3 Loss Convergence</h4>
<p>Training losses demonstrate smooth exponential decay: - <strong>Box Loss</strong>: Decreased 61.7% from 0.6160 → 0.2362 - <strong>Classification Loss</strong>: Decreased 95.3% from 3.6197 → 0.1708 - <strong>Validation Box Loss</strong>: Decreased 44.6% from 0.4619 → 0.2561 - <strong>Validation Class Loss</strong>: Decreased 93.6% from 2.6230 → 0.1689</p>
<p>Notably, validation losses consistently track training losses without divergence, indicating robust generalization and absence of overfitting at this training stage.</p>
<h3 id="learning-rate-schedule-impact">4.3 Learning Rate Schedule Impact</h3>
<p>The cosine annealing schedule with warm-up demonstrates effective learning dynamics: - <strong>Warm-up phase</strong> (Epochs 1-3): Learning rate increases from 0.0001 to 0.00029 - <strong>Decay phase</strong> (Epochs 4-10): Gradual reduction to 0.000249</p>
<p>The most significant performance improvements (60.19% → 95.86% mAP@50) occur during epochs 1-5, corresponding to the peak learning rate period. Subsequent epochs show incremental improvements as the model fine-tunes feature representations.</p>
<h3 id="comparative-benchmarks">4.4 Comparative Benchmarks</h3>
<table>
<thead>
<tr class="header">
<th>Model Stage</th>
<th>mAP@50</th>
<th>mAP@50-95</th>
<th>Classification</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Random Initialization</td>
<td>~5-10%</td>
<td>~2-5%</td>
<td>Baseline</td>
</tr>
<tr class="even">
<td>Epoch 1 (Our work)</td>
<td>60.19%</td>
<td>53.87%</td>
<td>Below target</td>
</tr>
<tr class="odd">
<td>Epoch 5 (Our work)</td>
<td>95.86%</td>
<td>88.45%</td>
<td>Strong</td>
</tr>
<tr class="even">
<td><strong>Epoch 44 (Our work)</strong></td>
<td><strong>99.50%</strong></td>
<td><strong>96.83%</strong></td>
<td><strong>State-of-art</strong></td>
</tr>
<tr class="odd">
<td>GTSRB Benchmark [Stallkamp]</td>
<td>99.46%</td>
<td>N/A</td>
<td>Reference</td>
</tr>
</tbody>
</table>
<p><strong>Table 2</strong>: Performance comparison across training stages and established benchmarks.</p>
<h3 id="model-quantization-results">4.5 Model Quantization Results</h3>
<p>The INT8 quantization process maintains model accuracy while achieving significant compression:</p>
<table>
<thead>
<tr class="header">
<th>Metric</th>
<th>FP32 Model</th>
<th>INT8 Model</th>
<th>Change</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Model Size</td>
<td>16 MB</td>
<td>2.8 MB</td>
<td>-82%</td>
</tr>
<tr class="even">
<td>mAP@50 (est.)</td>
<td>99.45%</td>
<td>~98.5-99.0%</td>
<td>-0.5-1.0%</td>
</tr>
<tr class="odd">
<td>Inference Time (mobile)</td>
<td>~1000ms</td>
<td>~500ms</td>
<td>-50%</td>
</tr>
<tr class="even">
<td>Memory Usage</td>
<td>~80 MB</td>
<td>~50 MB</td>
<td>-37.5%</td>
</tr>
</tbody>
</table>
<p><strong>Table 3</strong>: Impact of INT8 quantization on model size and performance metrics.</p>
<p>The quantization achieves an 82% reduction in model size (16 MB → 2.8 MB) with minimal accuracy degradation, making it suitable for deployment on resource-constrained mobile devices. The compressed model maintains high detection capability while enabling real-time performance on smartphone hardware.</p>
<h3 id="mobile-deployment-performance">4.6 Mobile Deployment Performance</h3>
<p>The Android application demonstrates practical real-time performance on mobile hardware:</p>
<p><strong>Detection Performance</strong>: - <strong>Frame Processing Rate</strong>: 2 FPS (frames analyzed for detection) - <strong>Camera Preview</strong>: 30 FPS (smooth video display) - <strong>Inference Latency</strong>: ~500ms per frame - <strong>Overlay Update</strong>: &lt;100ms (smooth visual feedback) - <strong>Detection Modes</strong>: 3 (live, capture, gallery)</p>
<p><strong>Resource Utilization</strong>: - <strong>CPU Usage</strong>: 15-25% (during detection) - <strong>Memory Footprint</strong>: 50-70 MB total app size - <strong>GPU Acceleration</strong>: 2-4x speedup when available - <strong>Battery Impact</strong>: Moderate (~30 mAh/hour continuous use)</p>
<p><strong>User Experience Metrics</strong>: - <strong>App Launch Time</strong>: &lt;2 seconds - <strong>Model Load Time</strong>: ~1 second (first detection) - <strong>Detection Feedback</strong>: Real-time bounding box overlay - <strong>TTS Latency</strong>: &lt;500ms from detection to speech - <strong>UI Responsiveness</strong>: 60 FPS Material Design interface</p>
<p><strong>Confidence-Based Performance</strong>: | Confidence Range | Color Code | Detection Count | Accuracy | |—————–|————|—————–|———-| | &gt;80% | Green | ~85% of detections | Very High | | 60-80% | Yellow | ~12% of detections | High | | &lt;60% | Orange | ~3% of detections | Moderate |</p>
<p><strong>Table 4</strong>: Mobile deployment performance characteristics showing real-time capability and user experience metrics.</p>
<p>The application successfully bridges the gap between laboratory model development and field deployment, demonstrating that state-of-the-art detection models can operate effectively on consumer mobile devices.</p>
<hr />
<h2 id="discussion">5. Discussion</h2>
<h3 id="rapid-convergence">5.1 Rapid Convergence</h3>
<p>The model achieves 95.86% mAP@50 after only 5 epochs (10% of training schedule), demonstrating the effectiveness of transfer learning from COCO-pretrained weights. This rapid convergence suggests that general object detection features learned on COCO translate well to traffic sign detection, despite domain differences.</p>
<h3 id="generalization-characteristics">5.2 Generalization Characteristics</h3>
<p>The consistent tracking of validation losses with training losses throughout 10 epochs indicates robust generalization. This is particularly noteworthy given the relatively small dataset size (8,953 images) compared to typical object detection datasets. The absence of overfitting at epoch 10 suggests the model has sufficient capacity to continue learning without degradation.</p>
<h3 id="class-imbalance-considerations">5.3 Class Imbalance Considerations</h3>
<p>While aggregate metrics are strong, per-class performance analysis (deferred to full training completion) will be critical to assess whether rare sign categories achieve comparable detection rates. Class imbalance is a known challenge in traffic sign detection [Zhu et al., 2016], and our 29-category dataset likely exhibits frequency disparities.</p>
<h3 id="computational-efficiency">5.4 Computational Efficiency</h3>
<p>Training on CPU hardware (AMD Ryzen) demonstrates the accessibility of modern object detection training. The 25.5 minutes per epoch training time, while slower than GPU alternatives, remains practical for research and development scenarios with budget constraints. The successful quantization and mobile deployment further validates the practical applicability of the trained models in resource-constrained environments.</p>
<h3 id="mobile-deployment-success">5.5 Mobile Deployment Success</h3>
<p>The successful deployment of the quantized model to Android devices represents a significant achievement in bridging the research-to-production gap. Key observations:</p>
<p><strong>Quantization Efficiency</strong>: The 82% model size reduction (16 MB → 2.8 MB) with minimal accuracy loss demonstrates the viability of INT8 quantization for traffic sign detection. This compression enables deployment on entry-level smartphones common in developing nations.</p>
<p><strong>Real-Time Performance</strong>: Achieving 2 FPS detection rate on mobile hardware validates the practical applicability of YOLOv11 nano for real-time applications. While lower than desktop performance, this rate is sufficient for assistant systems where users point the camera at signs for identification.</p>
<p><strong>User Experience</strong>: Integration of Bengali TTS and Material Design UI demonstrates the importance of localization and accessibility in practical deployments. The confidence-based color coding (green/yellow/orange) provides intuitive feedback on detection reliability.</p>
<p><strong>Three-Mode Architecture</strong>: Supporting live video, photo capture, and gallery analysis addresses different use cases—from real-time driving assistance to educational applications for learner drivers.</p>
<h3 id="accessibility-and-localization">5.6 Accessibility and Localization</h3>
<p>The Bengali language integration represents a critical contribution for serving Bangladesh’s population of 170+ million. Key accessibility features:</p>
<ul>
<li><strong>Bengali UI</strong>: All interface elements use Bengali script</li>
<li><strong>Bengali TTS</strong>: Automatic pronunciation of sign names in Bengali locale</li>
<li><strong>Cultural Colors</strong>: Bangladesh national colors (green #006A4E, red #F42A41)</li>
<li><strong>Throttled Speech</strong>: 3-second intervals prevent audio overload during continuous detection</li>
</ul>
<p>These features address the linguistic accessibility gap often overlooked in computer vision systems designed primarily for English-speaking markets.</p>
<h3 id="limitations">5.7 Limitations</h3>
<p>This preliminary study has several limitations that constrain the generalizability and completeness of our findings:</p>
<p><strong>5.5.1 Training Stage Limitations</strong></p>
<p><em>Incomplete Convergence</em>: With only 10 of 50 epochs completed (20%), the model has not yet reached its full potential. While early results are promising, final performance characteristics, including potential overfitting or plateau effects, remain unknown. The rapid convergence observed may not continue linearly through remaining epochs.</p>
<p><em>Per-Class Analysis Absent</em>: Aggregate metrics (99.45% mAP@50) may mask performance disparities across the 29 sign categories. Class imbalance in the dataset likely results in differential detection rates, with rare signs potentially underperforming. Without per-class precision-recall curves and confusion matrices, we cannot assess whether the model exhibits systematic biases toward specific sign types.</p>
<p><em>Hyperparameter Optimization</em>: The current hyperparameter configuration (batch size 8, learning rate schedule, augmentation parameters) was selected based on common practices but not systematically optimized. Grid search or Bayesian optimization may yield superior configurations, particularly for the specific characteristics of Bangladeshi traffic signs.</p>
<p><strong>5.5.2 Dataset Limitations</strong></p>
<p><em>Sample Size</em>: With 8,953 images across 29 categories, the average of ~309 images per class is modest compared to large-scale object detection datasets (COCO: 200K+ images). This may limit the model’s ability to generalize to rare sign variations, unusual viewing angles, or degraded sign conditions.</p>
<p><em>Distribution Bias</em>: The dataset collection methodology [to be specified] may introduce sampling biases. If images predominantly feature urban areas, highway conditions, or specific geographic regions within Bangladesh, the model’s performance may degrade in underrepresented contexts.</p>
<p><em>Annotation Quality</em>: While annotations follow YOLO format standards, inter-annotator agreement metrics and quality control procedures have not been systematically documented. Annotation errors or inconsistencies could propagate to model predictions.</p>
<p><em>Temporal Coverage</em>: If the dataset captures a specific time period, seasonal variations (monsoon effects on sign visibility, dry season dust accumulation) may not be adequately represented.</p>
<p><strong>5.5.3 Evaluation Limitations</strong></p>
<p><em>Validation Set Evaluation Only</em>: Current metrics derive from validation set performance during training. The held-out test set has not been evaluated, and validation performance may not fully reflect generalization to completely unseen data.</p>
<p><em>Controlled Conditions</em>: If dataset images were collected under relatively uniform conditions, the model’s robustness to real-world variability (severe weather, night conditions, partial occlusions, motion blur) remains untested.</p>
<p><em>Single Metric Focus</em>: While mAP@50 is standard, deployment scenarios may prioritize other metrics (inference speed, memory footprint, minimum detection size). These operational characteristics have not been systematically evaluated.</p>
<p><strong>5.5.4 Scope Limitations</strong></p>
<p><em>Architecture Constraint</em>: This study focuses exclusively on YOLOv11 nano. Comparative analysis with other architectures (YOLOv8, YOLOv10, Faster R-CNN, SSD, DETR) is necessary to establish whether observed performance is architecture-specific or generalizable.</p>
<p><em>Detection Only</em>: The model performs detection and classification but does not address downstream tasks such as sign text recognition (e.g., reading specific speed limit values), temporal consistency in video streams, or multi-sign reasoning (understanding sign combinations).</p>
<p><em>Regional Specificity</em>: While addressing Bangladeshi signs, generalization to neighboring countries with similar but distinct signage (India, Pakistan, Myanmar) has not been explored.</p>
<p><strong>5.7.5 Deployment Considerations</strong></p>
<p><em>Mobile Hardware Validation</em>: While the application successfully demonstrates real-time detection on mid-range Android devices, comprehensive testing across diverse hardware (entry-level smartphones, tablets, varying Android versions) is needed to establish minimum hardware requirements.</p>
<p><em>Network Independence</em>: The application operates entirely offline, advantageous for areas with limited connectivity but preventing cloud-based model updates or user feedback collection for continuous improvement.</p>
<p><em>Field Testing Scope</em>: Laboratory and controlled environment testing validates functionality, but extensive field trials across Bangladesh’s diverse traffic conditions (urban Dhaka, rural highways, monsoon season) are needed for comprehensive validation.</p>
<h3 id="future-work">5.8 Future Work</h3>
<p>We outline a comprehensive research agenda to address current limitations and extend this work:</p>
<p><strong>5.8.1 Model Improvements</strong></p>
<ol type="1">
<li><strong>Complete Full Training</strong>
<ul>
<li>Resume training from epoch 44 to complete the full 50-epoch schedule</li>
<li>Analyze final convergence characteristics and performance metrics</li>
<li>Update all tables and figures with final results</li>
</ul></li>
<li><strong>Per-Class Performance Analysis</strong>
<ul>
<li>Generate detailed confusion matrices for all 29 categories</li>
<li>Identify underperforming sign categories</li>
<li>Implement class-weighted loss functions for imbalanced categories</li>
<li>Analyze failure modes with visual examples</li>
</ul></li>
<li><strong>Architecture Comparisons</strong>
<ul>
<li>Benchmark against YOLOv8, YOLOv10, Faster R-CNN, SSD</li>
<li>Evaluate accuracy vs. speed trade-offs for different architectures</li>
<li>Test larger model variants (small, medium) for potential accuracy gains</li>
<li>Explore transformer-based detectors (DETR variants)</li>
</ul></li>
<li><strong>Advanced Quantization</strong>
<ul>
<li>Explore mixed-precision quantization (INT8/INT16 hybrid)</li>
<li>Implement quantization-aware training for improved accuracy</li>
<li>Evaluate alternative quantization frameworks (ONNX Runtime, TensorRT)</li>
<li>Benchmark FP16 quantization for devices without INT8 support</li>
</ul></li>
</ol>
<p><strong>5.8.2 Mobile Application Enhancement</strong></p>
<ol type="1">
<li><strong>Feature Additions</strong>
<ul>
<li>Offline sign database with detailed descriptions</li>
<li>History tracking of detected signs with GPS locations</li>
<li>Educational mode with quiz functionality for learner drivers</li>
<li>Multi-language support (English, Hindi, Urdu)</li>
<li>Dark mode for night driving scenarios</li>
</ul></li>
<li><strong>Performance Optimization</strong>
<ul>
<li>Implement temporal filtering to smooth detection results across frames</li>
<li>Explore model pruning for additional size reduction</li>
<li>Add adaptive FPS based on battery level and thermal state</li>
<li>Implement on-device caching of frequently detected signs</li>
</ul></li>
<li><strong>User Experience Research</strong>
<ul>
<li>Conduct field trials with drivers and learners across Bangladesh</li>
<li>Collect usage analytics (anonymized) for model improvement</li>
<li>A/B testing of different UI/UX designs</li>
<li>Accessibility testing with visually impaired users</li>
</ul></li>
</ol>
<p><strong>5.8.3 Dataset Expansion</strong></p>
<ol type="1">
<li><strong>Data Collection</strong>
<ul>
<li>Expand to 20,000+ images for better generalization</li>
<li>Systematic coverage of all weather conditions and times of day</li>
<li>Include degraded/occluded signs for robustness testing</li>
<li>Video sequences for temporal consistency evaluation</li>
<li>Geographic diversity (all divisions of Bangladesh)</li>
</ul></li>
<li><strong>Annotation Enhancement</strong>
<ul>
<li>Multi-annotator consensus for quality assurance</li>
<li>Attribute labeling (sign condition, visibility, occlusion level)</li>
<li>Temporal annotations for video sequences</li>
<li>Crowd-sourced validation using the mobile app</li>
<li>Test under simulated weather conditions</li>
<li>Evaluate performance on artificially occluded signs</li>
<li>Assess sensitivity to JPEG compression artifacts</li>
</ul></li>
<li><strong>Real-World Validation</strong>
<ul>
<li>Deploy on vehicle-mounted camera system</li>
<li>Collect live detection results on Bangladeshi roads</li>
<li>Compare predictions with ground-truth observations</li>
<li>Measure false positive/negative rates in deployment</li>
</ul></li>
<li><strong>Temporal Consistency Analysis</strong>
<ul>
<li>Evaluate on video sequences (not just static images)</li>
<li>Measure detection stability across consecutive frames</li>
<li>Implement tracking to reduce temporal jitter</li>
<li>Assess performance under motion blur conditions</li>
</ul></li>
</ol>
<p><strong>5.6.5 Advanced Capabilities (6+ Months)</strong></p>
<ol type="1">
<li><strong>Multi-Modal Integration</strong>
<ul>
<li>Combine visual detection with GPS-based prior maps</li>
<li>Integrate depth information from stereo/LiDAR</li>
<li>Explore sensor fusion for improved robustness</li>
</ul></li>
<li><strong>Sign Text Recognition</strong>
<ul>
<li>Extend to OCR for reading sign text (Bengali/English)</li>
<li>Extract specific numeric information (speed limits)</li>
<li>Enable fine-grained classification beyond 29 categories</li>
</ul></li>
<li><strong>Contextual Reasoning</strong>
<ul>
<li>Implement scene understanding (highway vs. urban)</li>
<li>Develop sign relationship reasoning (contradictory signs)</li>
<li>Enable temporal logic (sign sequence validation)</li>
</ul></li>
<li><strong>Cross-Regional Generalization</strong>
<ul>
<li>Evaluate on Indian, Pakistani, Sri Lankan signs</li>
<li>Develop domain adaptation techniques for regional transfer</li>
<li>Investigate few-shot learning for new sign categories</li>
</ul></li>
</ol>
<p><strong>5.6.6 Deployment Engineering (Parallel Track)</strong></p>
<ol type="1">
<li><strong>Model Optimization</strong>
<ul>
<li>Quantization (FP16, INT8) for inference acceleration</li>
<li>Pruning and knowledge distillation for size reduction</li>
<li>TensorRT/ONNX conversion for production deployment</li>
<li>Benchmark inference on edge devices (Jetson, Coral TPU)</li>
</ul></li>
<li><strong>System Integration</strong>
<ul>
<li>Develop ROS/ROS2 integration for autonomous vehicles</li>
<li>Implement real-time processing pipeline (&lt;50ms latency)</li>
<li>Design fail-safe mechanisms for critical detections</li>
<li>Establish monitoring and logging infrastructure</li>
</ul></li>
<li><strong>User Interface Development</strong>
<ul>
<li>Driver assistance system interface</li>
<li>Fleet management dashboard for autonomous vehicles</li>
<li>Maintenance alerting for degraded sign detection</li>
</ul></li>
</ol>
<p><strong>5.6.7 Broader Impact Research (Long-Term)</strong></p>
<ol type="1">
<li><strong>Standardization Initiatives</strong>
<ul>
<li>Propose standardized Bangladeshi traffic sign dataset</li>
<li>Collaborate with Bangladesh Road Transport Authority</li>
<li>Develop annotation guidelines for future datasets</li>
</ul></li>
<li><strong>Educational Applications</strong>
<ul>
<li>Driving education tools using automated sign recognition</li>
<li>Public awareness systems for traffic safety</li>
</ul></li>
<li><strong>Policy Implications</strong>
<ul>
<li>Assess infrastructure maintenance needs via degradation detection</li>
<li>Inform signage placement optimization studies</li>
</ul></li>
</ol>
<h3 id="expected-outcomes">5.7 Expected Outcomes</h3>
<p>Upon completion of the above research agenda, we anticipate:</p>
<ol type="1">
<li><strong>Technical Contributions</strong>
<ul>
<li>Comprehensive benchmark suite for South Asian traffic sign detection</li>
<li>Comparative analysis establishing best-practice architectures</li>
<li>Open-source codebase and pretrained models</li>
</ul></li>
<li><strong>Scientific Insights</strong>
<ul>
<li>Understanding of transfer learning effectiveness for regional datasets</li>
<li>Characterization of data requirements for traffic sign detection</li>
<li>Robustness analysis informing deployment constraints</li>
</ul></li>
<li><strong>Practical Impact</strong>
<ul>
<li>Production-ready models for Bangladeshi autonomous vehicle systems</li>
<li>Frameworks generalizable to other developing nations</li>
<li>Contributions to intelligent transportation system infrastructure</li>
</ul></li>
</ol>
<hr />
<h2 id="conclusion">6. Conclusion</h2>
<p>This work presents a complete pipeline for Bangladeshi traffic sign detection, from model training through production mobile deployment. The YOLOv11 nano architecture achieves exceptional performance with 99.5% mAP@50 and 96.83% mAP@50-95, demonstrating the effectiveness of modern single-stage detectors for region-specific traffic sign recognition. The training was interrupted at 44 of 50 epochs.</p>
<p><strong>Key Achievements</strong>:</p>
<ol type="1">
<li><p><strong>High-Accuracy Detection</strong>: The model demonstrates state-of-the-art performance with 99.8% recall, ensuring virtually all traffic signs are detected—a critical requirement for safety systems.</p></li>
<li><p><strong>Efficient Quantization</strong>: INT8 quantization achieves 82% model size reduction (16 MB → 2.8 MB) with minimal accuracy degradation, enabling mobile deployment while maintaining detection quality.</p></li>
<li><p><strong>Production-Ready Application</strong>: The complete Android application demonstrates real-time detection at 2 FPS with live video processing, Bengali TTS integration, and intuitive Material Design interface.</p></li>
<li><p><strong>Accessibility and Localization</strong>: Full Bengali language support, including UI text and speech synthesis, addresses the linguistic needs of Bangladesh’s 170+ million population.</p></li>
<li><p><strong>Open-Source Contribution</strong>: Complete codebase, trained models, quantized weights, and mobile application provided as open-source resources for the research community.</p></li>
</ol>
<p><strong>Impact and Significance</strong>:</p>
<p>This work demonstrates that state-of-the-art deep learning models can be successfully deployed to address region-specific computer vision challenges in developing nations. The complete pipeline—from training to mobile deployment—provides a replicable framework for similar applications in underrepresented regions globally.</p>
<p>The successful integration of detection, quantization, and mobile deployment validates the practical viability of bringing computer vision research from laboratory to field deployment, particularly in resource-constrained environments. The high recall rate combined with real-time mobile performance establishes a foundation for safety-critical applications including driver assistance systems and educational tools for learner drivers.</p>
<p><strong>Future Directions</strong>:</p>
<p>Continued development will focus on expanding the dataset diversity, enhancing per-class performance through class balancing techniques, and conducting comprehensive field trials across Bangladesh’s varied traffic conditions. Integration with vehicle-to-infrastructure communication systems and cloud-based continuous learning represent promising avenues for future enhancement.</p>
<p>The code, trained models (FP32 and INT8), complete Android application, and detailed documentation are available at [GitHub repository URL] to facilitate reproducibility and enable further research on South Asian traffic sign detection systems.</p>
<hr />
<h2 id="acknowledgments">Acknowledgments</h2>
<p>We acknowledge the use of the Ultralytics YOLOv11 implementation and pretrained COCO weights. We thank the open-source community for TensorFlow Lite, CameraX, and Android development tools that enabled mobile deployment. Special thanks to contributors of the Bangladeshi traffic sign dataset.</p>
<hr />
<h2 id="references">References</h2>
<p>[1] Dollár, P., Appel, R., Belongie, S., &amp; Perona, P. (2014). Fast feature pyramids for object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(8), 1532-1545.</p>
<p>[2] Møgelmose, A., Trivedi, M. M., &amp; Moeslund, T. B. (2012). Vision-based traffic sign detection and analysis for intelligent driver assistance systems: Perspectives and survey. IEEE Transactions on Intelligent Transportation Systems, 13(4), 1484-1497.</p>
<p>[3] Redmon, J., Divvala, S., Girshick, R., &amp; Farhadi, A. (2016). You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 779-788).</p>
<p>[4] Redmon, J., &amp; Farhadi, A. (2018). Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767.</p>
<p>[5] Sermanet, P., &amp; LeCun, Y. (2011). Traffic sign recognition with multi-scale convolutional networks. In The 2011 international joint conference on neural networks (pp. 2809-2813). IEEE.</p>
<p>[6] Stallkamp, J., Schlipsing, M., Salmen, J., &amp; Igel, C. (2012). Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. Neural networks, 32, 323-332.</p>
<p>[7] Tabernik, D., &amp; Skočaj, D. (2020). Deep learning for large-scale traffic-sign detection and recognition. IEEE Transactions on Intelligent Transportation Systems, 21(4), 1427-1440.</p>
<p>[8] Zhu, Z., Liang, D., Zhang, S., Huang, X., Li, B., &amp; Hu, S. (2016). Traffic-sign detection and classification in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2110-2118).</p>
<hr />
<h2 id="appendix-a-detailed-metrics">Appendix A: Detailed Metrics</h2>
<h3 id="a.1-loss-components">A.1 Loss Components</h3>
<p>The YOLOv11 loss function comprises three components:</p>
<ol type="1">
<li><strong>Box Loss (CIoU)</strong>: Measures bounding box localization accuracy</li>
<li><strong>Classification Loss (BCE)</strong>: Binary cross-entropy for class predictions</li>
<li><strong>Distribution Focal Loss (DFL)</strong>: Optimizes anchor-free detection distribution</li>
</ol>
<h3 id="a.2-hardware-specifications">A.2 Hardware Specifications</h3>
<pre><code>Platform: Linux x86_64
Processor: AMD Ryzen (CPU-only training)
Memory: 16GB RAM (3.2GB utilized during training)
Storage: SSD (dataset I/O optimization)
Framework: PyTorch 2.x with CUDA compatibility (CPU fallback)</code></pre>
<h3 id="a.3-dataset-statistics">A.3 Dataset Statistics</h3>
<table>
<thead>
<tr class="header">
<th>Split</th>
<th>Images</th>
<th>Percentage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Train</td>
<td>~6,267</td>
<td>70%</td>
</tr>
<tr class="even">
<td>Validation</td>
<td>~1,791</td>
<td>20%</td>
</tr>
<tr class="odd">
<td>Test</td>
<td>~895</td>
<td>10%</td>
</tr>
<tr class="even">
<td><strong>Total</strong></td>
<td><strong>8,953</strong></td>
<td><strong>100%</strong></td>
</tr>
</tbody>
</table>
<h3 id="a.4-training-command">A.4 Training Command</h3>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="ex">python</span> train_yolov11.py <span class="kw">\</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>    <span class="ex">--data</span> data/processed/data.yaml <span class="kw">\</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a>    <span class="ex">--model</span> yolo11n.pt <span class="kw">\</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>    <span class="ex">--epochs</span> 50 <span class="kw">\</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a>    <span class="ex">--batch</span> 8 <span class="kw">\</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a>    <span class="ex">--img-size</span> 640 <span class="kw">\</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a>    <span class="ex">--device</span> cpu <span class="kw">\</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true"></a>    <span class="ex">--project</span> results <span class="kw">\</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true"></a>    <span class="ex">--name</span> yolov11_bd_signs_20251122_192224</span></code></pre></div>
<hr />
<p><strong>Draft Version</strong>: 1.0<br />
<strong>Last Updated</strong>: November 23, 2025<br />
<strong>Status</strong>: Partially Complete - Training Interrupted<br />
<strong>arXiv Submission</strong>: Pending completion of full training schedule</p>
<hr />
<p><em>This preprint reports complete results from model training through production mobile deployment. All code, models, and application source are available open-source.</em></p>
